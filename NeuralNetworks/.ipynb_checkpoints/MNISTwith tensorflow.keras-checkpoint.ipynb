{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 20s 336us/step - loss: 0.2600 - acc: 0.9246\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.1050 - acc: 0.9674\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.0723 - acc: 0.9769\n",
      "10000/10000 [==============================] - 0s 48us/step\n",
      "0.11655401694215835\n",
      "0.9649\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.python.keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = tf.keras.utils.normalize(x_train,axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test,axis=1)\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential()  # a basic feed-forward model\n",
    "model.add(tf.keras.layers.Flatten())  # takes our 28x28 and makes it 1x784\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))  # our output layer. 10 units for 10 classes. Softmax for probability distribution\n",
    "\n",
    "model.compile(optimizer='adam',  # Good default optimizer to start with\n",
    "              loss='sparse_categorical_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
    "              metrics=['accuracy'])  # what to track\n",
    "\n",
    "model.fit(x_train, y_train, epochs=3)  # train the model\n",
    "\n",
    "val_loss, val_acc = model.evaluate(x_test, y_test)  # evaluate the out of sample data with model\n",
    "print(val_loss)  # model's loss (error)\n",
    "print(val_acc)  # model's accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A sequential model is what you're going to use most of the time. It just means things are going to go in direct order. A feed forward model. No going backwards...for now.\n",
    "\n",
    "\n",
    "```\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "```\n",
    "-- This will serve as our input layer. It's going to take the data we throw at it, and just flatten it for us. Next, we want our hidden layers. We're going to go with the simplest neural network layer, which is just a Dense layer. This refers to the fact that it's a densely-connected layer, meaning it's \"fully connected,\" where each node connects to each prior and subsequent node. \n",
    "\n",
    "\n",
    "```\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "```\n",
    "This layer has 128 units. The activation function is relu, short for rectified linear. Currently, relu is the activation function you should just default to. There are many more to test for sure, but, if you don't know what to use, use relu to start.\n",
    "\n",
    "Let's add another identical layer for good measure.\n",
    "\n",
    "```\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "```\n",
    "Now, we're ready for an output layer:\n",
    "\n",
    "```\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "```\n",
    "This is our final layer. It has 10 nodes. 1 node per possible number prediction. In this case, our activation function is a softmax function, since we're really actually looking for something more like a probability distribution of which of the possible prediction options this thing we're passing features through of is. Great, our model is done.\n",
    "\n",
    "Now we need to \"compile\" the model. This is where we pass the settings for actually optimizing/training the model we've defined.\n",
    "\n",
    "\n",
    "```\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "\n",
    "Loss is a calculation of error. A neural network doesn't actually attempt to maximize accuracy. It attempts to minimize loss. Again, there are many choices, but some form of categorical crossentropy is a good start for a classification task like this.\n",
    "```\n",
    "val_loss, val_acc = model.evaluate(x_test, y_test)  \n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 36us/step\n",
      "0.11655401694215835\n",
      "0.9649\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(val_loss)\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a high accuracy and low loss might mean your model learned how to classify digits in general (it generalized)...or it simply memorized every single example you showed it (it overfit). This is why we need to test on out-of-sample data (data we didn't use to train the model).\n",
    "\n",
    "```\n",
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(val_loss)\n",
    "print(val_acc)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    filepath='./model/epic_num_reader.h5',\n",
    "    overwrite=True,\n",
    "    include_optimizer=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model(compile=False,filepath='./model/epic_num_reader.h5')\n",
    "#from keras.models import save_model, load_model\n",
    "\n",
    "# Creates a HDF5 file 'my_model.h5' \n",
    "#save_model(model, 'my_model.h5') # model, [path + \"/\"] name of model\n",
    "\n",
    "# Deletes the existing model\n",
    "#del model  \n",
    "\n",
    "# Returns a compiled model identical to the previous one\n",
    "#new_model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.30126272e-08 1.15734565e-07 4.15042086e-05 ... 9.99604642e-01\n",
      "  1.14031318e-08 1.97752543e-05]\n",
      " [3.56072359e-11 1.26427185e-04 9.99869704e-01 ... 5.13319776e-10\n",
      "  6.24919894e-09 9.00507357e-14]\n",
      " [3.97666220e-08 9.99770701e-01 8.46077382e-05 ... 5.38379390e-05\n",
      "  5.66790332e-05 1.10003396e-06]\n",
      " ...\n",
      " [2.78582157e-10 5.10919875e-08 8.19836998e-08 ... 4.87155512e-06\n",
      "  8.72908430e-08 3.27246562e-05]\n",
      " [2.74906597e-06 1.45601675e-07 4.95203608e-07 ... 4.04836953e-07\n",
      "  1.21105666e-04 1.04161870e-07]\n",
      " [2.33929245e-06 5.39617595e-08 1.47764445e-06 ... 2.97358721e-10\n",
      "  4.77572314e-07 3.83337238e-08]]\n"
     ]
    }
   ],
   "source": [
    "predictions = new_model.predict(x_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.argmax(predictions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6ef40bff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11215074 0.55234025 0.57241636 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.08650726 0.16103093\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.20086699 0.42853985 0.57241636 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.17580507 0.3240499\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.00271221 0.25610542 0.49996316 0.5870937  0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.30696124 0.3240499\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.03661479 0.42516847 0.38568586 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.3097518  0.3240499\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.24816691 0.42516847 0.29759712 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.13572983 0.3418432  0.3240499\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.26850846 0.42516847 0.13332351 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.35407783 0.3544007  0.3240499\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.02599851 0.31325987 0.42516847 0.06904253 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.46915312 0.3544007  0.23856435\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.18425032 0.34445024 0.36156059 0.03809243 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.46915312 0.3544007  0.13319843\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.05387565 0.34280637 0.33442521\n",
      "  0.28033177 0.34445024 0.15232414 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.46915312 0.3544007  0.16898308\n",
      "  0.         0.         0.         0.1504842  0.17713556 0.41246938\n",
      "  0.51449576 0.60385769 0.86000157 0.93512735 0.93275221 0.33630401\n",
      "  0.27241918 0.34173804 0.06695566 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.44259728 0.35300542 0.47116458\n",
      "  0.76003359 0.76003359 0.76003359 0.81005324 0.91821292 0.88894262\n",
      "  0.85749293 0.79709215 0.51029139 0.35019173 0.11161138 0.00939397\n",
      "  0.26337622 0.33902583 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.16603812 0.35188241\n",
      "  0.64988379 0.64988379 0.64988379 0.56671709 0.35427113 0.19912315\n",
      "  0.         0.         0.         0.         0.         0.19163692\n",
      "  0.28711399 0.29834273 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31751607\n",
      "  0.28711399 0.18578615 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31751607\n",
      "  0.28711399 0.07729789 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31751607\n",
      "  0.28711399 0.07729789 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31751607\n",
      "  0.28824436 0.12747371 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31751607\n",
      "  0.28711399 0.13018592 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31751607\n",
      "  0.28711399 0.20748381 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31751607\n",
      "  0.28824436 0.20748381 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.18036416\n",
      "  0.28711399 0.20748381 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#plt.imshow(x_train[2])\n",
    "plt.imshow(x_train[2],cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "print(x_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Currently `save` requires model to be a graph network. Consider using `save_weights`, in order to save the weights of the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-7b7b789f82a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epic_num_reader.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ML/mlenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       raise NotImplementedError(\n\u001b[0;32m-> 1359\u001b[0;31m           \u001b[0;34m'Currently `save` requires model to be a graph network. Consider '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m           'using `save_weights`, in order to save the weights of the model.')\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Currently `save` requires model to be a graph network. Consider using `save_weights`, in order to save the weights of the model."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
