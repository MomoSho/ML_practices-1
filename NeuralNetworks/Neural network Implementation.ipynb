{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "import matplotlib.pyplot as plt \n",
    "plt.gray() \n",
    "plt.matshow(digits.images[1]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The code above prints (1797, 64) to show the shape of input data matrix and the pixelated digit “1” in the image above.  The code we are going to write in this neural networks tutorial will try and estimate the digits that these pixels represent (using neural networks of course). First things first, we need to get the input data in shape. To do so, we need to do two things:\n",
    "\n",
    "    Scale the data\n",
    "    Split the data into test and train sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Scaling data</h2>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need to scale the input data?  First, have a look at one of the dataset pixel representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-f6a1ee5621dc>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-f6a1ee5621dc>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Out[2]:array([  0.,   0.,   5.,  13.,   9.,   1.,   0.,   0.,   0.,   0.,  13., 15.,  10.,  15.,   5.,   0.,   0.,   3.,  15.,   2.,   0.,  11.,8.,   0.,   0.,   4.,  12.,   0.,   0.,   8.,   8.,   0.,   0.,5.,   8.,   0.,   0.,   9.,   8.,   0.,   0.,   4.,  11.,0.,1.,  12.,   7.,   0.,   0.,   2.,  14.,   5.,  10.,  12.,   0.,0.,   0.,   0.,   6.,  13.,  10.,   0.,  0.,  0.])\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "digits.data[0,:]\n",
    "Out[2]:array([  0.,   0.,   5.,  13.,   9.,   1.,   0.,   0.,   0.,   0.,  13., 15.,  10.,  15.,   5.,   0.,   0.,   3.,  15.,   2.,   0.,  11.,8.,   0.,   0.,   4.,  12.,   0.,   0.,   8.,   8.,   0.,   0.,5.,   8.,   0.,   0.,   9.,   8.,   0.,   0.,   4.,  11.,0.,1.,  12.,   7.,   0.,   0.,   2.,  14.,   5.,  10.,  12.,   0.,0.,   0.,   0.,   6.,  13.,  10.,   0.,  0.,  0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the input data ranges from 0 up to 15?  It’s standard practice to scale the input data so that it all fits mostly between either 0 to 1 or with a small range centred around 0 i.e. -1 to 1.  Why?  Well, it can help the convergence of the neural network and is especially important if we are combining different data types.  Thankfully, this is easily done using sci-kit learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-164756e4cd41>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-164756e4cd41>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    Out[3]:array([ 0.        , -0.33501649, -0.04308102,  0.27407152, -0.66447751,\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scale = StandardScaler()\n",
    "X = X_scale.fit_transform(digits.data)\n",
    "X[0,:]\n",
    "Out[3]:array([ 0.        , -0.33501649, -0.04308102,  0.27407152, -0.66447751,\n",
    "       -0.84412939, -0.40972392, -0.12502292, -0.05907756, -0.62400926,\n",
    "        0.4829745 ,  0.75962245, -0.05842586,  1.12772113,  0.87958306,\n",
    "       -0.13043338, -0.04462507,  0.11144272,  0.89588044, -0.86066632,\n",
    "       -1.14964846,  0.51547187,  1.90596347, -0.11422184, -0.03337973,\n",
    "        0.48648928,  0.46988512, -1.49990136, -1.61406277,  0.07639777,\n",
    "        1.54181413, -0.04723238,  0.        ,  0.76465553,  0.05263019,\n",
    "       -1.44763006, -1.73666443,  0.04361588,  1.43955804,  0.        ,\n",
    "       -0.06134367,  0.8105536 ,  0.63011714, -1.12245711, -1.06623158,\n",
    "        0.66096475,  0.81845076, -0.08874162, -0.03543326,  0.74211893,\n",
    "        1.15065212, -0.86867056,  0.11012973,  0.53761116, -0.75743581,\n",
    "       -0.20978513, -0.02359646, -0.29908135,  0.08671869,  0.20829258,\n",
    "       -0.36677122, -1.14664746, -0.5056698 , -0.19600752])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit learn standard scaler by default normalises the data by subtracting the mean and dividing by the standard deviation.  As can be observed, most of the data points are centered around zero and contained within -2 and 2.  This is a good starting point.  There is no real need to scale the output data y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Creating test and training datasets</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In machine learning, there is a phenomenon called “overfitting”. This occurs when models, during training, become too complex – they become really well adapted to predict the training data, but when they are asked to predict something based on new data that they haven’t “seen” before, they perform poorly. In other words, the models don’t generalise very well. To make sure that we are not creating models which are too complex, it is common practice to split the dataset into a training set and a test set. The training set is, obviously, the data that the model will be trained on, and the test set is the data that the model will be tested on after it has been trained. The amount of training data is always more numerous than the testing data, and is usually between 60-80% of the total dataset.</p>\n",
    "<br>\n",
    "<p>Again, scikit learn makes this splitting of the data into training and testing sets easy:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0c13c1c4adcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we’ve made the test set to be 40% of the total data, leaving 60% to train with. The train_test_split function in scikit learn pushes the data randomly into the different datasets – in other words, it doesn’t take the first 60% of rows as the training set and the second 40% of rows as the test set. This avoids data collection artefacts from degrading the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Setting up the output layer</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you would have been able to gather, we need the output layer to predict whether the digit represented by the input pixels is between 0 and 9. Therefore, a sensible neural network architecture would be to have an output layer of 10 nodes, with each of these nodes representing a digit from 0 to 9. We want to train the network so that when, say, an image of the digit “5” is presented to the neural network, the node in the output layer representing 5 has the highest value. Ideally, we would want to see an output looking like this: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]. However, in reality, we can settle for something like this: [0.01, 0.1, 0.2, 0.05, 0.3, 0.8, 0.4, 0.03, 0.25, 0.02]. In this case, we can take the maximum index of the output array and call that our predicted digit.\n",
    "\n",
    "For the MNIST data supplied in the scikit learn dataset, the “targets” or the classification of the handwritten digits is in the form of a single number. We need to convert that single number into a vector so that it lines up with our 10 node output layer. In other words, if the target value in the dataset is “1” we want to convert it into the vector: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]. The code below does just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-aaad0cfdf395>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-aaad0cfdf395>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    Out[8]:\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def convert_y_to_vect(y):\n",
    "    y_vect = np.zeros((len(y), 10))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, y[i]] = 1\n",
    "    return y_vect\n",
    "y_v_train = convert_y_to_vect(y_train)\n",
    "y_v_test = convert_y_to_vect(y_test)\n",
    "y_train[0], y_v_train[0]\n",
    "Out[8]:\n",
    "(1, array([ 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed above, the MNIST target (1) has been converted into the vector [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], which is what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creating the neural network</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to specify the structure of the neural network. For the input layer, we know we need 64 nodes to cover the 64 pixels in the image. As discussed, we need 10 output layer nodes to predict the digits. We’ll also need a hidden layer in our network to allow for the complexity of the task. Usually, the number of hidden layer nodes is somewhere between the number of input layers and the number of output layers. Let’s define a simple Python list that designates the structure of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_structure = [64, 30, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use sigmoid activation functions again, so let’s setup the sigmoid function and its derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def f_deriv(x):\n",
    "    return f(x) * (1 - f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we now have an idea of what our neural network will look like. How do we train it? Remember the algorithm from Section 4.9 , which we’ll repeat here for ease of access and review:\n",
    "\n",
    "Randomly initialise the weights for each layer W(l)\n",
    "\n",
    "While iterations < iteration limit:\n",
    "1. Set ΔW and Δb to zero\n",
    "2. For samples 1 to m:\n",
    "a. Perform a feed foward pass through all the nl layers. Store the activation function outputs h(l)\n",
    "b. Calculate the δ(nl) value for the output layer\n",
    "c. Use backpropagation to calculate the δ(l) values for layers 2 to nl−1\n",
    "d. Update the ΔW(l) and Δb(l) for each layer\n",
    "3. Perform a gradient descent step using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align} \n",
    "W^{(l)} = W^{(l)} – \\alpha \\left[\\frac{1}{m} \\Delta W^{(l)} \\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first step is to initialise the weights for each layer. To make it easy to organise the various layers, we’ll use Python dictionary objects (initialised by {}). Finally, the weights have to be initialised with random values – this is to ensure that the neural network will converge correctly during training. We use the numpy library random_sample function to do this. The weight initialisation code is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as r\n",
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {}\n",
    "    b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1]))\n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The next step is to set the mean accumulation values ΔW\n",
    "and Δb to zero (they need to be the same size as the weight and bias matrices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "        tri_b[l] = np.zeros((nn_structure[l],))\n",
    "    return tri_W, tri_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now step into the gradient descent loop, the first step is to perform a feed forward pass through the networkm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(x, W, b):\n",
    "    h = {1: x}\n",
    "    z = {}\n",
    "    for l in range(1, len(W) + 1):\n",
    "        # if it is the first layer, then the input into the weights is x, otherwise, \n",
    "        # it is the output from the last layer\n",
    "        if l == 1:\n",
    "            node_in = x\n",
    "        else:\n",
    "            node_in = h[l]\n",
    "        z[l+1] = W[l].dot(node_in) + b[l] # z^(l+1) = W^(l)*h^(l) + b^(l)  \n",
    "        h[l+1] = f(z[l+1]) # h^(l) = f(z^(l)) \n",
    "    return h, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to then calculate the output layer delta δ(nl) and any hidden layer delta values δ(l) to perform the backpropagation pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_out_layer_delta(y, h_out, z_out):\n",
    "    # delta^(nl) = -(y_i - h_i^(nl)) * f'(z_i^(nl))\n",
    "    return -(y-h_out) * f_deriv(z_out)\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "    return np.dot(np.transpose(w_l), delta_plus_1) * f_deriv(z_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put all the steps together into the final function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    m = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        for i in range(len(y)):\n",
    "            delta = {}\n",
    "            # perform the feed forward pass and return the stored h and z values, to be used in the\n",
    "            # gradient descent step\n",
    "            h, z = feed_forward(X[i, :], W, b)\n",
    "            # loop from nl-1 to 1 backpropagating the errors\n",
    "            for l in range(len(nn_structure), 0, -1):\n",
    "                if l == len(nn_structure):\n",
    "                    delta[l] = calculate_out_layer_delta(y[i,:], h[l], z[l])\n",
    "                    avg_cost += np.linalg.norm((y[i,:]-h[l]))\n",
    "                else:\n",
    "                    if l > 1:\n",
    "                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(h^(l))\n",
    "                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(h[l][:,np.newaxis])) \n",
    "                    # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                    tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            W[l] += -alpha * (1.0/m * tri_W[l])\n",
    "            b[l] += -alpha * (1.0/m * tri_b[l])\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0/m * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above deserves a bit of explanation. First, we aren’t setting a termination of the gradient descent process based on some change or precision of the cost function. Rather, we are just running it for a set number of iterations (3,000 in this case) and we’ll monitor how the average cost function changes as we progress through the training (avg_cost_func list in the above code). In each iteration of the gradient descent, we cycle through each training sample (range(len(y)) and perform the feed forward pass and then the backpropagation. The backpropagation step is an iteration through the layers starting at the output layer and working backwards – range(len(nn_structure), 0, -1). We calculate the average cost, which we are tracking during the training, at the output layer (l == len(nn_structure)). We also update the mean accumulation values, ΔW and Δb, designated as tri_W and tri_b, for every layer apart from the output layer (there are no weights connecting the output layer to any further layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
